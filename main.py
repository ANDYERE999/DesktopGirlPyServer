import gradio as gr
import psutil
import platform
import GPUtil
import os
import threading
import time
from datetime import datetime
import requests
import socket

def get_system_info():
    """è·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯"""
    try:
        # CPUä¿¡æ¯
        cpu_info = {
            "å¤„ç†å™¨": platform.processor(),
            "CPUæ ¸å¿ƒæ•°": psutil.cpu_count(logical=False),
            "é€»è¾‘æ ¸å¿ƒæ•°": psutil.cpu_count(logical=True),
            "CPUé¢‘ç‡": f"{psutil.cpu_freq().current:.2f} MHz" if psutil.cpu_freq() else "æœªçŸ¥"
        }
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        memory_info = {
            "æ€»å†…å­˜": f"{memory.total / (1024**3):.2f} GB",
            "å¯ç”¨å†…å­˜": f"{memory.available / (1024**3):.2f} GB",
            "å†…å­˜ä½¿ç”¨ç‡": f"{memory.percent}%"
        }
        
        # GPUä¿¡æ¯
        gpu_info = {}
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                for i, gpu in enumerate(gpus):
                    gpu_info[f"GPU {i}"] = {
                        "åç§°": gpu.name,
                        "æ˜¾å­˜æ€»é‡": f"{gpu.memoryTotal} MB",
                        "æ˜¾å­˜ä½¿ç”¨": f"{gpu.memoryUsed} MB",
                        "æ˜¾å­˜ä½¿ç”¨ç‡": f"{gpu.memoryUtil * 100:.1f}%",
                        "GPUä½¿ç”¨ç‡": f"{gpu.load * 100:.1f}%",
                        "æ¸©åº¦": f"{gpu.temperature}Â°C"
                    }
            else:
                gpu_info["GPUçŠ¶æ€"] = "æœªæ£€æµ‹åˆ°GPUæˆ–é©±åŠ¨é—®é¢˜"
        except:
            gpu_info["GPUçŠ¶æ€"] = "GPUä¿¡æ¯è·å–å¤±è´¥"
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = {
            "æ“ä½œç³»ç»Ÿ": f"{platform.system()} {platform.release()}",
            "ç³»ç»Ÿç‰ˆæœ¬": platform.version(),
            "æœºå™¨ç±»å‹": platform.machine(),
            "Pythonç‰ˆæœ¬": platform.python_version()
        }
        
        return cpu_info, memory_info, gpu_info, system_info
    except Exception as e:
        return {}, {}, {}, {"é”™è¯¯": f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}"}

def get_server_resource_usage():
    """è·å–æœåŠ¡ç«¯èµ„æºå ç”¨æƒ…å†µ"""
    try:
        # è·å–å½“å‰è¿›ç¨‹
        current_process = psutil.Process(os.getpid())
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = current_process.cpu_percent()
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_info = current_process.memory_info()
        memory_mb = memory_info.rss / (1024 * 1024)
        memory_percent = current_process.memory_percent()
        
        # çº¿ç¨‹æ•°
        thread_count = current_process.num_threads()
        
        # æ–‡ä»¶å¥æŸ„æ•°
        try:
            file_handles = current_process.num_fds() if hasattr(current_process, 'num_fds') else len(current_process.open_files())
        except:
            file_handles = "è·å–å¤±è´¥"
        
        # è¿è¡Œæ—¶é—´
        create_time = datetime.fromtimestamp(current_process.create_time())
        uptime = datetime.now() - create_time
        
        server_info = {
            "è¿›ç¨‹ID": current_process.pid,
            "CPUä½¿ç”¨ç‡": f"{cpu_percent:.2f}%",
            "å†…å­˜ä½¿ç”¨": f"{memory_mb:.2f} MB",
            "å†…å­˜ä½¿ç”¨ç‡": f"{memory_percent:.2f}%",
            "çº¿ç¨‹æ•°": thread_count,
            "æ–‡ä»¶å¥æŸ„æ•°": file_handles,
            "è¿è¡Œæ—¶é—´": str(uptime).split('.')[0],
            "å¯åŠ¨æ—¶é—´": create_time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return server_info
    except Exception as e:
        return {"é”™è¯¯": f"è·å–æœåŠ¡ç«¯èµ„æºä¿¡æ¯å¤±è´¥: {str(e)}"}

def format_info_display(info_dict, title):
    """æ ¼å¼åŒ–ä¿¡æ¯æ˜¾ç¤º"""
    if not info_dict:
        return f"## {title}\næš‚æ— æ•°æ®"
    
    result = f"## {title}\n"
    for key, value in info_dict.items():
        if isinstance(value, dict):
            result += f"### {key}\n"
            for sub_key, sub_value in value.items():
                result += f"- **{sub_key}**: {sub_value}\n"
        else:
            result += f"- **{key}**: {value}\n"
    return result

def get_recommended_configs():
    """è·å–æ¨èé…ç½®"""
    try:
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system = platform.system()
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ä¸­å›½å¤§é™†ï¼ˆç®€å•çš„IPåœ°ç†ä½ç½®æ£€æµ‹ï¼‰
        is_china = check_if_in_china()
        
        recommendations = {
            "gpt_sovits": False,
            "api_llm": True,  # é»˜è®¤æ¨èAPI
            "api_llm_china": is_china,
            "local_7b": False,
            "local_14b": False,
            "local_27b": False,
            "local_32b": False
        }
        
        if system == "Darwin":  # macOS
            # Macä½¿ç”¨ç»Ÿä¸€å†…å­˜
            if memory_gb >= 31.5:  # >=32GB
                recommendations["gpt_sovits"] = True
                recommendations["local_27b"] = True
                recommendations["local_32b"] = True
                recommendations["local_14b"] = True
                recommendations["local_7b"] = True
            elif memory_gb >= 15.5:  # >=16GB
                recommendations["gpt_sovits"] = True
                recommendations["local_14b"] = True
                recommendations["local_7b"] = True
        
        elif system == "Windows":  # Windows
            # æ£€æŸ¥NVIDIAæ˜¾å¡
            try:
                gpus = GPUtil.getGPUs()
                has_nvidia = len(gpus) > 0
                
                if has_nvidia:
                    max_gpu_memory = max([gpu.memoryTotal for gpu in gpus]) / 1024  # è½¬æ¢ä¸ºGB
                    
                    if max_gpu_memory >= 23.5:  # >=24GB
                        recommendations["local_27b"] = True
                        recommendations["local_32b"] = True
                        recommendations["local_14b"] = True
                        recommendations["local_7b"] = True
                    elif max_gpu_memory >= 11.5:  # >=12GB
                        recommendations["local_14b"] = True
                        recommendations["local_7b"] = True
                    elif max_gpu_memory >= 7.5:  # >=8GB
                        recommendations["local_7b"] = True
                
                # GPT-Sovitsåªéœ€è¦å†…å­˜>=8GB
                if memory_gb >= 7.5:  # >=8GB
                    recommendations["gpt_sovits"] = True
                    
            except:
                # å¦‚æœè·å–GPUä¿¡æ¯å¤±è´¥ï¼Œåªæ£€æŸ¥å†…å­˜
                if memory_gb >= 7.5:
                    recommendations["gpt_sovits"] = True
        
        return recommendations
    
    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›ä¿å®ˆçš„æ¨è
        return {
            "gpt_sovits": False,
            "api_llm": True,
            "api_llm_china": True,
            "local_7b": False,
            "local_14b": False,
            "local_27b": False,
            "local_32b": False
        }

def check_if_in_china():
    """æ£€æŸ¥æ˜¯å¦åœ¨ä¸­å›½å¤§é™†"""
    try:
        # å°è¯•è®¿é—®ä¸€ä¸ªå›½å¤–ç½‘ç«™æ¥åˆ¤æ–­ç½‘ç»œç¯å¢ƒ
        response = requests.get('http://httpbin.org/ip', timeout=3)
        ip_info = response.json()
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åœ°ç†ä½ç½®æ£€æµ‹é€»è¾‘
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æ£€æŸ¥ä¸€äº›å¸¸è§çš„ä¸­å›½IPæ®µæˆ–ä½¿ç”¨å…¶ä»–æ–¹æ³•
        
        # ä¹Ÿå¯ä»¥å°è¯•è®¿é—®Googleæ¥åˆ¤æ–­
        try:
            socket.create_connection(("8.8.8.8", 53), timeout=3)
            return False  # èƒ½è®¿é—®Googleï¼Œå¯èƒ½ä¸åœ¨ä¸­å›½å¤§é™†
        except:
            return True   # ä¸èƒ½è®¿é—®Googleï¼Œå¯èƒ½åœ¨ä¸­å›½å¤§é™†
            
    except:
        # ç½‘ç»œæ£€æµ‹å¤±è´¥ï¼Œé»˜è®¤è¿”å›Trueï¼ˆä¿å®ˆå¤„ç†ï¼‰
        return True

def format_recommendations(recommendations):
    """æ ¼å¼åŒ–æ¨èé…ç½®æ˜¾ç¤º"""
    checkmark = "âœ…"
    cross = "âŒ"
    
    result = "## æ ¹æ®æ‚¨çš„è®¾å¤‡é…ç½®ï¼Œä»¥ä¸‹æ˜¯æ¨èè®¾ç½®ï¼š\n\n"
    
    # GPT-Sovitsæ¨è
    gpt_sovits_status = checkmark if recommendations["gpt_sovits"] else cross
    result += f"{gpt_sovits_status} æ‚¨çš„è®¾å¤‡å¯ä»¥å¼€å¯GPT-Sovits\n\n"
    
    # APIè°ƒç”¨æ¨è
    api_status = checkmark if recommendations["api_llm"] else cross
    api_china_note = "ï¼ˆéœ€è¦ä»£ç†æˆ–å›½å†…APIï¼‰" if recommendations["api_llm_china"] else ""
    result += f"{api_status} æ‚¨çš„è®¾å¤‡å¯ä»¥ä½¿ç”¨APIè°ƒç”¨LLM {api_china_note}\n\n"
    
    # æœ¬åœ°æ¨¡å‹æ¨è
    models = [
        ("local_7b", "7B"),
        ("local_14b", "14B"), 
        ("local_27b", "27B"),
        ("local_32b", "32B")
    ]
    
    for key, size in models:
        status = checkmark if recommendations[key] else cross
        result += f"{status} æ‚¨çš„è®¾å¤‡å¯ä»¥ä½¿ç”¨{size}æœ¬åœ°LLMæ¨¡å‹\n\n"
    
    # æ·»åŠ é¢å¤–è¯´æ˜
    result += "---\n\n"
    result += "**è¯´æ˜ï¼š**\n"
    result += "- âœ… è¡¨ç¤ºæ¨èä½¿ç”¨è¯¥é…ç½®\n"
    result += "- âŒ è¡¨ç¤ºä¸æ¨èæˆ–è®¾å¤‡æ€§èƒ½ä¸è¶³\n"
    result += "- å³ä½¿æœ¬åœ°æ€§èƒ½è¶³å¤Ÿï¼Œä»å»ºè®®ä¼˜å…ˆè€ƒè™‘APIè°ƒç”¨ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ\n"
    
    if recommendations["api_llm_china"]:
        result += "- æ£€æµ‹åˆ°æ‚¨å¯èƒ½åœ¨ä¸­å›½å¤§é™†ï¼Œä½¿ç”¨APIæ—¶å¯èƒ½éœ€è¦é…ç½®ä»£ç†æˆ–ä½¿ç”¨å›½å†…APIæœåŠ¡\n"
    
    return result

def update_performance_and_recommendations():
    """æ›´æ–°æ€§èƒ½ä¿¡æ¯å’Œæ¨èé…ç½®"""
    # è·å–æ€§èƒ½ä¿¡æ¯
    cpu_info, memory_info, gpu_info, system_info = get_system_info()
    server_info = get_server_resource_usage()
    
    # è·å–æ¨èé…ç½®
    recommendations = get_recommended_configs()
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    system_display = format_info_display(system_info, "ç³»ç»Ÿä¿¡æ¯")
    cpu_display = format_info_display(cpu_info, "å¤„ç†å™¨ä¿¡æ¯")
    memory_display = format_info_display(memory_info, "å†…å­˜ä¿¡æ¯")
    gpu_display = format_info_display(gpu_info, "æ˜¾å¡ä¿¡æ¯")
    server_display = format_info_display(server_info, "æœåŠ¡ç«¯èµ„æºå ç”¨")
    recommendations_display = format_recommendations(recommendations)
    
    return system_display, cpu_display, memory_display, gpu_display, server_display, recommendations_display

with gr.Blocks() as mainUI:
    gr.Markdown("DesktopGirlæœåŠ¡ç«¯")
    with gr.Tabs():
        with gr.Tab("Live2Dæ¨¡å‹é…ç½®"):
            pass
        with gr.Tab("LLMæ¨¡å‹é…ç½®"):
            pass
        with gr.Tab("GPT-Sovitsæ¨¡å‹é…ç½®"):
            pass
        with gr.Tab("æ€§èƒ½ç›‘æµ‹å™¨"):
            gr.Markdown("### ç”µè„‘é…ç½®ä¸æœåŠ¡ç«¯æ€§èƒ½ç›‘æµ‹")
            
            with gr.Row():
                update_btn = gr.Button("ğŸ”„ åˆ·æ–°ä¿¡æ¯", variant="primary")
                auto_refresh = gr.Checkbox(label="è‡ªåŠ¨åˆ·æ–° (æ¯5ç§’)", value=False)
            
            with gr.Row():
                with gr.Column():
                    system_output = gr.Markdown("æ­£åœ¨åŠ è½½ç³»ç»Ÿä¿¡æ¯...")
                    cpu_output = gr.Markdown("æ­£åœ¨åŠ è½½CPUä¿¡æ¯...")
                    memory_output = gr.Markdown("æ­£åœ¨åŠ è½½å†…å­˜ä¿¡æ¯...")
                
                with gr.Column():
                    gpu_output = gr.Markdown("æ­£åœ¨åŠ è½½GPUä¿¡æ¯...")
                    server_output = gr.Markdown("æ­£åœ¨åŠ è½½æœåŠ¡ç«¯ä¿¡æ¯...")

            with gr.Row():
                gr.Markdown("### æ¨èä½¿ç”¨çš„é…ç½®")
                recommended_config = gr.Markdown("æ­£åœ¨åˆ†ææ‚¨çš„è®¾å¤‡é…ç½®...")

            
            # æ›´æ–°æŒ‰é’®äº‹ä»¶
            update_btn.click(
                fn=update_performance_and_recommendations,
                outputs=[system_output, cpu_output, memory_output, gpu_output, server_output, recommended_config]
            )
            
            # åˆå§‹åŠ è½½
            mainUI.load(
                fn=update_performance_and_recommendations,
                outputs=[system_output, cpu_output, memory_output, gpu_output, server_output, recommended_config]
            )
        
        with gr.Tab("å®¢æˆ·ç«¯ç‹¬ç«‹æ‰“åŒ…"):
            pass

if __name__ == "__main__":
    mainUI.launch()
